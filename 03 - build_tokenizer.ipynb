{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46c5a26-4d39-455a-98cb-e1b57d24cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import *\n",
    "import datasets\n",
    "from datasets import *\n",
    "import project_paths as pp\n",
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers, models, trainers, processors, decoders\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f52becb0-c819-4e12-aadd-525a872d5180",
   "metadata": {},
   "source": [
    "Steps to build a tokenizer:\n",
    "* Normalization\n",
    "* Pre-tokenization\n",
    "* Model\n",
    "* Post-processor\n",
    "* Decoder\n",
    "Refer to [this page](https://huggingface.co/learn/nlp-course/en/chapter6/8#building-a-tokenizer-block-by-block) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8305ff9d-6d8b-4dd6-abbc-caa9712b1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece_tokenizer(corpus: List[str], vocab_size: int = 4096) -> Tokenizer:\n",
    "    '''Train a WordPiece tokenizer on a text corpus.\n",
    "\n",
    "    Args:\n",
    "        corpus: List of strings containing the training texts\n",
    "        vocab_size: Size of vocabulary to learn (default: 4096)\n",
    "\n",
    "    Returns:\n",
    "        Tokenizer: Trained WordPiece tokenizer with the following components:\n",
    "            - Normalizer: Strips whitespace, lowercases, removes accents\n",
    "            - Pre-tokenizer: Splits on whitespace\n",
    "            - Model: WordPiece with [UNK] token\n",
    "            - Post-processor: Adds special tokens [BOS], [EOS], [CLS]\n",
    "            - Decoder: WordPiece with '##' prefix\n",
    "    '''\n",
    "    # Helper function to yield batches of the corpus for training\n",
    "    def training_corpus_iterator(batch_size=512):\n",
    "        for i in range(0, len(corpus), batch_size):\n",
    "            yield corpus[i: i + batch_size]\n",
    "\n",
    "    # Initialize tokenizer with WordPiece model and [UNK] token for unknown words\n",
    "    # The WordPiece model will learn subword units by breaking words into commonly occurring pieces\n",
    "    tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\n",
    "    \n",
    "    # Set up normalizer sequence to clean and standardize text:\n",
    "    # - Strip whitespace from both ends\n",
    "    # - Convert to lowercase for consistency \n",
    "    # - NFD unicode normalization to separate characters and diacritics\n",
    "    # - Remove accent marks while preserving base characters\n",
    "    tokenizer.normalizer = normalizers.Sequence(\n",
    "        [\n",
    "            normalizers.Strip(left=True, right=True),\n",
    "            normalizers.Lowercase(),\n",
    "            normalizers.NFD(),\n",
    "            normalizers.StripAccents()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Configure pre-tokenizer to split text on whitespace boundaries\n",
    "    # This creates initial word-level tokens before WordPiece subword tokenization\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "        [\n",
    "            pre_tokenizers.Whitespace()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Define special tokens used for various purposes:\n",
    "    # [UNK] - Unknown tokens not in vocabulary\n",
    "    # [BOS] - Beginning of sequence marker\n",
    "    # [EOS] - End of sequence marker\n",
    "    # [PAD] - Padding token for fixed length\n",
    "    special_tokens = ['[UNK]', '[BOS]', '[EOS]', '[PAD]']\n",
    "    \n",
    "    # Set up WordPiece trainer with:\n",
    "    # - Target vocabulary size\n",
    "    # - Special tokens to reserve in vocabulary\n",
    "    trainer = trainers.WordPieceTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "    \n",
    "    # Train on batches of text using iterator\n",
    "    tokenizer.train_from_iterator(training_corpus_iterator(), trainer=trainer)\n",
    "    \n",
    "    # Add post-processor to wrap sequences with special tokens:\n",
    "    # [BOS] at start, [EOS] at end\n",
    "    # Maps special tokens to their vocabulary IDs\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=f'[BOS]:0 $A:0 [EOS]:0',\n",
    "        special_tokens=[(special_token, tokenizer.token_to_id(special_token)) for special_token in special_tokens],\n",
    "    )\n",
    "    \n",
    "    # Configure WordPiece decoder with '##' prefix\n",
    "    # This helps reconstruct original text by marking subword continuations\n",
    "    tokenizer.decoder = decoders.WordPiece(prefix='##')\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811978ab-6e42-48d6-b311-076fbb087155",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_folder_path = os.path.join(pp.aclImdb_dataset_folder_path, 'train')\n",
    "train_dataset = datasets.load_from_disk(train_dataset_folder_path)\n",
    "corpus_size = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42f2da4-5efd-4977-9f4a-b3fdc234ca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Compilers\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set vocabulary size to 4096 tokens\n",
    "# This determines how many unique subword tokens the WordPiece tokenizer will learn\n",
    "# Smaller vocab = more subword splitting but smaller model\n",
    "# Larger vocab = less splitting but larger model\n",
    "vocab_size = 4096\n",
    "word_piece_tokenizer = train_word_piece_tokenizer(train_dataset['text'])\n",
    "word_piece_fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=word_piece_tokenizer,\n",
    "    unk_token='[UNK]',\n",
    "    bos_token='[BOS]',\n",
    "    eos_token='[EOS]',\n",
    "    pad_token='[PAD]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d6dcc8-a4c7-45f3-8500-484fe76bacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained tokenizer to disk and reload it\n",
    "# This ensures the tokenizer can be reused without retraining\n",
    "# The tokenizer is saved with the vocabulary size in the folder name\n",
    "tokenizer_folder_path = os.path.join(pp.word_piece_tokenizer_folder_path, str(vocab_size))\n",
    "if not os.path.isdir(tokenizer_folder_path):\n",
    "    os.makedirs(tokenizer_folder_path)\n",
    "word_piece_fast_tokenizer.save_pretrained(tokenizer_folder_path)\n",
    "word_piece_fast_tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dca173-40cc-4756-95a6-03e6eb3a6b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
